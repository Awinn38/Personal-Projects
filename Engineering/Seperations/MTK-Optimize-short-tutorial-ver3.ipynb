{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  In this tutorial, several optimization problems are solved \r\n",
    "#  using the ModelingToolkit.jl and GalacticOptim.jl packages in Julia. \r\n",
    "\r\n",
    "#  The function to be optimized (minimized) is the Rosenbrock function:\r\n",
    "\r\n",
    "#       (a - x)^2 + b * (y - x^2)^2\r\n",
    "\r\n",
    "#  where x and y and variables and a and b are constant parameters.\r\n",
    "#  The above function is a common test function for optimization methods. \r\n",
    "#  By inspection, it can be see that the minimum is at x = a, y = a^2 \r\n",
    "#  and the function value is zero at this location. Despite having an\r\n",
    "#  obvious solution, this problem is still a useful numerical test case\r\n",
    "#  the problem is difficult to solve numerically if b = 100 or is greater.\r\n",
    "\r\n",
    "#  The packages ModelingToolkit.jl, GlacticOptim.jl, and Optim.jl\r\n",
    "#  need to be installed for this tutorial.\r\n",
    "\r\n",
    "#  Date:  9/12/2021\r\n",
    "\r\n",
    "#  Author:  Doug Frey, UMBC\r\n",
    "\r\n",
    "#  Julia version 1.6.1 was used to create this tutorial"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  The following package versions were used \r\n",
    "#  for this tutorial:\r\n",
    "\r\n",
    "#\r\n",
    "#      Status `C:\\Users\\Douglas Frey\\environment_v161_var1\\Project.toml`\r\n",
    "#   [a75be94c] GalacticOptim v2.0.3\r\n",
    "#   [961ee093] ModelingToolkit v6.4.9\r\n",
    "#   [429524aa] Optim v1.4.1\r\n",
    "#   [1dea7af3] OrdinaryDiffEq v5.55.1\r\n",
    "#   [91a5bcdd] Plots v1.21.3\r\n",
    "\r\n",
    "#  Package versions can be checked by running the following commands.\r\n",
    "\r\n",
    "#  First import Pkg:\r\n",
    "\r\n",
    "       import Pkg\r\n",
    "\r\n",
    "#  The command below is only needed if Julia 1.6.1 is being used in\r\n",
    "#  a Jupyter notebook due to a bug in Julia 1.6.1. This command can\r\n",
    "#  be deleted otherwise.\r\n",
    "\r\n",
    "       Pkg.DEFAULT_IO[] = stdout\r\n",
    "\r\n",
    "#  Write the status to the output:\r\n",
    "\r\n",
    "       Pkg.status()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#  Make available for use the needed packages:\r\n",
    "\r\n",
    "using ModelingToolkit, GalacticOptim, Optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#  Example problem 1 \r\n",
    "\r\n",
    "#  Minimize the Rosenbrock function\r\n",
    "#  with no constraints\r\n",
    "\r\n",
    "#   Newton Trust Region method is used\r\n",
    "\r\n",
    "#  Define the variables and parameters.\r\n",
    "\r\n",
    "@variables x y \r\n",
    "@parameters a b\r\n",
    "\r\n",
    "#  Define the objective function\r\n",
    "\r\n",
    "objective = (a - x)^2 + b * (y - x^2)^2\r\n",
    "\r\n",
    "#  Create an OptmizationSystem object and name it\r\n",
    "\r\n",
    "@named  sys = OptimizationSystem(objective, [x,y], [a,b])\r\n",
    "\r\n",
    "#  Set initial guess.  Note that on purpose a very bad \r\n",
    "#  initial guess will be used.\r\n",
    "\r\n",
    "u0 = [ x => -1.0, y => -2.0]\r\n",
    "\r\n",
    "#  Set parameters, Note the => operator which creates a\r\n",
    "#  pair object.\r\n",
    "\r\n",
    "params = [ a => 1.0, b => 100.0]\r\n",
    "\r\n",
    "#  Define the problem to solve.  Note that the Newton Trust\r\n",
    "#  Region method uses both the gradient and  Hessian matrix.\r\n",
    "#  These will be determined by automatic differentiation (not\r\n",
    "#  finite differences) by specifying grad=true, hess=true below.\r\n",
    "\r\n",
    "prob = OptimizationProblem(sys,u0, params, grad=true, hess=true)\r\n",
    "\r\n",
    "#  Create a callback function to monitor the progress to convergence\r\n",
    "\r\n",
    "callback = function (p,l)\r\n",
    "    println(\"Objective function value =  $l\")\r\n",
    "    return false\r\n",
    "end\r\n",
    "\r\n",
    "#  Use the Newton Trust Region method. Desired error is 10^-12. A maximum\r\n",
    "#  of 100 iterations will be performed with objective function value \r\n",
    "#  reported every 5 iterations.  Maximum time limit is 100 seconds:\r\n",
    "\r\n",
    "result = solve(prob, NewtonTrustRegion(), x_abstol = 1e-12, \r\n",
    "                   f_abstol = 1e-12, g_abstol = 1e-12, cb = callback, \r\n",
    "                    show_every = 5, time_limit = 100 , iterations=100)\r\n",
    "\r\n",
    "println(\" \")\r\n",
    "println(\"Final value of objective function = \", result.minimum)\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for x = \", result.minimizer[1])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for y = \", result.minimizer[2])\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Objective function value =  904.0\n",
      "Objective function value =  2.4036459427675534\n",
      "Objective function value =  0.8702016582178861\n",
      "Objective function value =  0.1175275677662976\n",
      "Objective function value =  0.00012257826985173384\n",
      " \n",
      "Final value of objective function = 0.0\n",
      " \n",
      "Final value for x = 1.0\n",
      " \n",
      "Final value for y = 1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Now examine some results symbolically\r\n",
    "#  to show symbolics ability of ModelingToolkit.jl\r\n",
    "\r\n",
    "#  First show the gradient\r\n",
    "\r\n",
    "Symbolics.gradient(objective,[x,y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Next show the Hessian matrix symbolically\r\n",
    "\r\n",
    "Symbolics.hessian(objective,[x,y])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Next show derivaitve of the objective_function\r\n",
    "#  with respect to x symbolically\r\n",
    "\r\n",
    "Dx = Differential(x)\r\n",
    "\r\n",
    "expand_derivatives(Dx(objective))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Example problem 2 \r\n",
    "\r\n",
    "#  Minimize the Rosenbrock function\r\n",
    "#  with no constraints\r\n",
    "\r\n",
    "#   The Broyden, Fletcher, Golfarb, Shanno (BFGS) \r\n",
    "#   Quasi-Newton method will be used\r\n",
    "\r\n",
    "#  Define the variables and parameters.\r\n",
    "\r\n",
    "@variables x y \r\n",
    "@parameters a b\r\n",
    "\r\n",
    "#  Define the objective function\r\n",
    "\r\n",
    "objective = (a - x)^2 + b * (y - x^2)^2\r\n",
    "\r\n",
    "#  Create an OptmizationSystem object and name it\r\n",
    "\r\n",
    "@named  sys = OptimizationSystem(objective, [x,y], [a,b])\r\n",
    "\r\n",
    "#  Set initial guess.  Note that on purpose a very bad \r\n",
    "#  initial guess will be used.\r\n",
    "\r\n",
    "u0 = [ x => -1.0, y=>-2.0]\r\n",
    "\r\n",
    "#  Set parameters\r\n",
    "\r\n",
    "params = [ a => 1.0, b => 100.0]\r\n",
    "\r\n",
    "#  Define the problem to solve.  Note that the BFGS method \r\n",
    "#  uses only the gradient (and not the Hessian matrix)\r\n",
    "#  since the Hessian is approximated internally by the method.\r\n",
    "\r\n",
    "prob = OptimizationProblem(sys,u0, params, grad=true, hess=false)\r\n",
    "\r\n",
    "#  Create a callback function to monitor the progress to convergence\r\n",
    "\r\n",
    "callback = function (p,l)\r\n",
    "    println(\"Objective function value =  $l\")\r\n",
    "    return false\r\n",
    "end\r\n",
    "\r\n",
    "#  Use the BFGS method. Desired error is 10^-12. A maximum\r\n",
    "#  of 100 iterations will be performed with objective function value \r\n",
    "#  reported every 5 iterations.  Maximum time limit is 100 seconds:\r\n",
    "\r\n",
    "result = solve(prob, BFGS(), x_abstol = 1e-12, \r\n",
    "                   f_abstol = 1e-12, g_abstol = 1e-12, cb = callback, \r\n",
    "                    show_every = 5, time_limit = 100 , iterations=100)\r\n",
    "\r\n",
    "println(\" \")\r\n",
    "println(\"Final value of objective function = \", result.minimum)\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for x = \", result.minimizer[1])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for y = \", result.minimizer[2])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Example problem 3 \r\n",
    "\r\n",
    "#  Minimize the Rosenbrock function\r\n",
    "#  with no constraints\r\n",
    "\r\n",
    "#  The Nelder-Mead method will be used. Note this method\r\n",
    "#  does not use either the gradient or Hessian and only\r\n",
    "#  uses function evaluations.\r\n",
    "\r\n",
    "#  Define the variables and parameters.\r\n",
    "\r\n",
    "@variables x y \r\n",
    "@parameters a b\r\n",
    "\r\n",
    "#  Remainder of statements are similar to the first two examples\r\n",
    "#  so most comments are eliminated below.\r\n",
    "\r\n",
    "objective = (a - x)^2 + b * (y - x^2)^2\r\n",
    "\r\n",
    "@named  sys = OptimizationSystem(objective, [x,y], [a,b])\r\n",
    "\r\n",
    "u0 = [ x => -1.0, y=>-2.0]\r\n",
    "\r\n",
    "params = [ a => 1.0, b => 100.0]\r\n",
    "\r\n",
    "#  The Nelder-Mead method does not use either the gradient or\r\n",
    "#  Hessian matrix.  Only function evaluations are used.\r\n",
    "\r\n",
    "prob = OptimizationProblem(sys,u0, params, grad=false, hess=false)\r\n",
    "\r\n",
    "callback = function (p,l)\r\n",
    "    println(\"Objective function value =  $l\")\r\n",
    "    return false\r\n",
    "end\r\n",
    "\r\n",
    "#  The Nelder Mead method will be used.  Since this method is less\r\n",
    "#  efficient per iteration, more iterations are needed than before.\r\n",
    "\r\n",
    "result = solve(prob, NelderMead(),x_abstol = 1e-12, \r\n",
    "                   f_abstol = 1e-12, g_abstol = 1e-12, cb = callback, \r\n",
    "                    show_every = 20, time_limit = 100 , iterations=200)\r\n",
    "\r\n",
    "println(\" \")\r\n",
    "println(\"Final value of objective function = \", result.minimum)\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for x = \", result.minimizer[1])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for y = \", result.minimizer[2])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Example problem 4\r\n",
    "\r\n",
    "#  Minimize the Rosenbrock function using\r\n",
    "#  Particle Swarm method so upper and lower \r\n",
    "#  limits can be used on the search variables.\r\n",
    "\r\n",
    "#  Note that the Particle Swarm method\r\n",
    "#  does not use either the gradient or Hessian and only\r\n",
    "#  uses function evaluations. The method permits upper and\r\n",
    "#  lower limits on the search variables. The method is \r\n",
    "#  loosely based on the behavior of flocking birds trying to\r\n",
    "#  find food.\r\n",
    "\r\n",
    "#  The Particle Swarm method is a global optimization method \r\n",
    "#  and not a local optimization method like the other options \r\n",
    "#  shown above.\r\n",
    "\r\n",
    "#  Define the variables and parameters.\r\n",
    "\r\n",
    "@variables x y \r\n",
    "@parameters a b\r\n",
    "\r\n",
    "#  Remainder of statements are similar to the first two examples\r\n",
    "#  so most comments are eliminated below.\r\n",
    "\r\n",
    "objective = (a - x)^2 + b * (y - x^2)^2\r\n",
    "\r\n",
    "@named  sys = OptimizationSystem(objective, [x,y], [a,b])\r\n",
    "\r\n",
    "u0 = [ x => -1.0, y=>-1.0]\r\n",
    "\r\n",
    "params = [ a => 1.0, b => 100.0]\r\n",
    "\r\n",
    "#  Niether gradient nor Hessian are needed\r\n",
    "\r\n",
    "prob = OptimizationProblem(sys, u0, params, grad=false, hess=false)\r\n",
    "\r\n",
    "callback = function (p,l)\r\n",
    "    println(\"Objective function value =  $l\")\r\n",
    "    return false\r\n",
    "end\r\n",
    "\r\n",
    "#  Define upper and lower limits for x and y.  First\r\n",
    "#  element in the two vectors below corresponds to x and\r\n",
    "#  the second element corresponds to y.\r\n",
    "\r\n",
    "lower_limit = [-5, -5]\r\n",
    "\r\n",
    "upper_limit = [5, 5]\r\n",
    "\r\n",
    "#  Particle Swarm method may needs lots of iterations. 100 search\r\n",
    "#  particles will be used here.\r\n",
    "\r\n",
    "         result = GalacticOptim.solve(prob, \r\n",
    "                    ParticleSwarm(lower = lower_limit, upper = upper_limit, \r\n",
    "                   n_particles = 100), x_abstol = 1e-12, f_abstol = 1e-12, \r\n",
    "                      g_abstol = 1e-12, cb = callback, \r\n",
    "                      show_every = 10, time_limit = 200, iterations=100);\r\n",
    "\r\n",
    "#  Note that, due to the constraints, the x=y=1 minimimum cannot be achieved.\r\n",
    "\r\n",
    "println(\" \")\r\n",
    "println(\"Final value of objective function = \", result.minimum)\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for x = \", result.minimizer[1])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for y = \", result.minimizer[2])"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Example problem 5.\r\n",
    "\r\n",
    "#  Minimize the Rosenbrock function\r\n",
    "#  with an equality constraint on x and y.\r\n",
    "\r\n",
    "#  The problem solved is:\r\n",
    "\r\n",
    "#       minimize:   (1 - x)^2 + 100 * (y - x^2)^2\r\n",
    "#       subject to:     x*y = 2.0\r\n",
    "\r\n",
    "#  Using the penalty function method, the above problem\r\n",
    "#  can be solved as follows:\r\n",
    "\r\n",
    "#    minimize (1-x)^2 + 100*(y-x^2)^2 + penalty_factor*(x*y-2.0)^2\r\n",
    "#       where  penalty_factor is a large number (e.g., 10^6)\r\n",
    "\r\n",
    "#  The BFGS method will be used. \r\n",
    "\r\n",
    "#  Define the variables and parameters.\r\n",
    "\r\n",
    "@variables x y \r\n",
    "@parameters a b penalty_factor\r\n",
    "\r\n",
    "#  Remainder of statements are similar to the first two examples\r\n",
    "#  so most comments are eliminated below.\r\n",
    "\r\n",
    "objective = (a - x)^2 + b * (y - x^2)^2  + penalty_factor*(x*y-2.0)^2\r\n",
    "\r\n",
    "@named  sys = OptimizationSystem(objective, [x,y], [a,b,penalty_factor])\r\n",
    "\r\n",
    "u0 = [ x => 1.0, y=> 1.0]\r\n",
    "\r\n",
    "params = [ a => 1.0, b => 100.0, penalty_factor => 10^6]\r\n",
    "\r\n",
    "prob = OptimizationProblem(sys,u0, params , grad=true, hess=false)\r\n",
    "\r\n",
    "callback = function (p,l)\r\n",
    "    println(\"Objective function value =  $l\")\r\n",
    "    return false\r\n",
    "end\r\n",
    "\r\n",
    "#  Broyden, Fletcher, Golfarb, Shanno (BFGS) method will be used\r\n",
    "\r\n",
    "result = solve(prob, BFGS(),x_abstol = 1e-12, \r\n",
    "                   f_abstol = 1e-12, g_abstol = 1e-12, cb = callback, \r\n",
    "                    show_every = 5, time_limit = 100 , iterations=100)\r\n",
    "\r\n",
    "#  Determine error in equality constraint\r\n",
    "\r\n",
    "error1 = result.minimizer[1]*result.minimizer[2] - 2.0\r\n",
    "\r\n",
    "println(\" \")\r\n",
    "println(\"Final value of objective function = \", result.minimum)\r\n",
    "println(\" \")\r\n",
    "println(\"Final error in eq. constraint = \", error1)\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for x = \", result.minimizer[1])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for y = \", result.minimizer[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Example problem 6 \r\n",
    "\r\n",
    "#  Minimize the Rosenbrock function with\r\n",
    "#  an inequality constraint on x and y.\r\n",
    "\r\n",
    "#  The problem solved is:\r\n",
    "\r\n",
    "#       minimize:   (1 - x)^2 + 100 * (y - x^2)^2\r\n",
    "#       subject to:     x^2 + y^2 is less than or equal to 0.5\r\n",
    "\r\n",
    "#  Using the penalty function method, the above problem\r\n",
    "#  can be solved as follows:\r\n",
    "\r\n",
    "#    minimize (1-x)^2 + 100*(y-x^2)^2 + penalty_factor*max(0,(x^2+y^2.0 - 0.5))^2\r\n",
    "#       where  penalty_factor is a large number (e.g., 10^6)\r\n",
    "\r\n",
    "#  The BFGS method will be used. \r\n",
    "\r\n",
    "#  Define the variables and parameters.\r\n",
    "\r\n",
    "@variables x y \r\n",
    "@parameters a b penalty_factor\r\n",
    "\r\n",
    "#  Remainder of statements are similar to the first two examples\r\n",
    "#  so most comments are eliminated below.\r\n",
    "\r\n",
    "objective = (a - x)^2 + b * (y - x^2)^2  + \r\n",
    "                   penalty_factor*(max(0,x^2 + y^2 - 0.5))^2\r\n",
    "\r\n",
    "@named  sys = OptimizationSystem(objective, [x,y], [a,b,penalty_factor])\r\n",
    "\r\n",
    "u0 = [ x => 0.0, y=> 0.0]\r\n",
    "\r\n",
    "params = [ a => 1.0, b => 100.0, penalty_factor => 10^6]\r\n",
    "\r\n",
    "prob = OptimizationProblem(sys,u0, params , grad=true, hess=false)\r\n",
    "\r\n",
    "callback = function (p,l)\r\n",
    "    println(\"Objective function value =  $l\")\r\n",
    "    return false\r\n",
    "end\r\n",
    "\r\n",
    "#  Broyden, Fletcher, Golfarb, Shanno (BFGS) method will be used\r\n",
    "\r\n",
    "result = solve(prob, BFGS(),x_abstol = 1e-12, \r\n",
    "                   f_abstol = 1e-12, g_abstol = 1e-12, cb = callback, \r\n",
    "                    show_every = 5, time_limit = 100 , iterations=100)\r\n",
    "\r\n",
    "#  Determine error in equality constraint\r\n",
    "\r\n",
    "x2_plus_y2 = result.minimizer[1]^2 + result.minimizer[2]^2  \r\n",
    "\r\n",
    "println(\" \")\r\n",
    "println(\"Final value of objective function = \", result.minimum)\r\n",
    "println(\" \")\r\n",
    "println(\"Final value of x2 + y2 = \", x2_plus_y2)\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for x = \", result.minimizer[1])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for y = \", result.minimizer[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Example problem 7 \r\n",
    "\r\n",
    "#  The Rosenbrock function is minimized as in \r\n",
    "#  example problem 2 except vector notation \r\n",
    "#  will be used here.  To better \r\n",
    "#  illustrate the use of vectors, two independent\r\n",
    "#  problems will be solved simultaneously using\r\n",
    "#  a single objective function.  Specifically, \r\n",
    "#  the vectors w and par1 are used for the first \r\n",
    "#  problem where w[1] = x,  w[2] = y, par1[1] = 1.0 \r\n",
    "#  and par1[2] = 100.0.  A similar correspondence applies \r\n",
    "#  to v and par2 for the second problem.\r\n",
    "\r\n",
    "#  Define the variables and parameters. \r\n",
    "\r\n",
    "@variables w[1:2]  v[1:2]\r\n",
    "@parameters par1[1:2] par2[1:2]\r\n",
    "\r\n",
    "#  Define the objective function\r\n",
    "\r\n",
    "objective = (par1[1] - w[1])^2 + par1[2] * (w[2] - w[1]^2)^2 +\r\n",
    "               (par2[1] - v[1])^2 + par2[2] * (v[2] - v[1]^2)^2\r\n",
    "\r\n",
    "#  Note that argument splatting (...) is used below\r\n",
    "\r\n",
    "@named  sys = OptimizationSystem(objective, [w...; v...], [par1...; par2...])\r\n",
    "\r\n",
    "#  Remainder of statements are similar to those above\r\n",
    "#  so most comments are eliminated below.\r\n",
    "\r\n",
    "w0 = [ w[1] => -1.0, w[2] => -2.0,  v[1] => -2.0, v[2] => -2.0]\r\n",
    "\r\n",
    "params = [par1[1] => 1.0, par1[2] => 100.0, par2[1] => 2.0, par2[2] => 100.0]\r\n",
    "\r\n",
    "prob = OptimizationProblem(sys, w0, params, grad=true, hess=false)\r\n",
    "\r\n",
    "callback = function (p,l)\r\n",
    "    println(\"Objective function value =  $l\")\r\n",
    "    return false\r\n",
    "end\r\n",
    "\r\n",
    "#  Broyden, Fletcher, Golfarb, Shanno (BFGS) method will be used\r\n",
    "\r\n",
    "result = solve(prob, BFGS(),x_abstol = 1e-12, \r\n",
    "                   f_abstol = 1e-12, g_abstol = 1e-12, cb = callback, \r\n",
    "                    show_every = 5, time_limit = 100 , iterations=100)\r\n",
    "\r\n",
    "println(\" \")\r\n",
    "println(\"Final value of objective function = \", result.minimum)\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for w[1] = \", result.minimizer[1])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for w[2] = \", result.minimizer[2])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for v[1] = \", result.minimizer[3])\r\n",
    "println(\" \")\r\n",
    "println(\"Final value for v[2] = \", result.minimizer[4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  Additional notes:\r\n",
    "\r\n",
    "#  1.  If there are no  parameters in the problem,\r\n",
    "#      then you can eliminate the @parameters statement. But\r\n",
    "#      you still  need to include an empty vector (i.e., [])\r\n",
    "#      elsewhere where the parameters are supposed to \r\n",
    "#      appear, such as in the following two statements:\r\n",
    "\r\n",
    "#   @named  sys = OptimizationSystem(objective, [x,y], [])\r\n",
    "#   prob = OptimizationProblem(sys,u0,[],grad=true,hess=false)\r\n",
    "\r\n",
    "#  2.  If you want to solve a system of algebraic equations\r\n",
    "#      such as:\r\n",
    "\r\n",
    "#             f(x,y) = 0   and g(x,y) = 0\r\n",
    "\r\n",
    "#      you can reformulate the above problem into the\r\n",
    "#      following equivalent optimization problem:\r\n",
    "\r\n",
    "#           minimize:    (f(x,y))^2 + (g(x,y))^2\r\n",
    "\r\n",
    "#      The above optimzation problem can be solved as shown\r\n",
    "#      by the examples above.\r\n",
    "\r\n",
    "# 3.   For large problems the BFGS method may use a large amount\r\n",
    "#      of memory.  In this case use the low-memory usage version of\r\n",
    "#      this algorithm by replacing BFGS() with LBFGS().\r\n",
    "\r\n",
    "# 4.   Note the following options for the function solve()\r\n",
    "#             x_abstol = error in independent variables\r\n",
    "#             f_abstol = error in the value of objective function\r\n",
    "#             g_abstol = error in the gradient (must be zero at min)\r\n",
    "\r\n",
    "# 5.   A summary of which optmization method to use for various\r\n",
    "#      problems is as follows: \r\n",
    "\r\n",
    "#      For a low-dimensional problem with analytic gradients and Hessians, \r\n",
    "#      use the Newton method with trust region. For larger problems or \r\n",
    "#      when there is no analytic Hessian, use LBFGS. If the function is \r\n",
    "#      non-differentiable, use Nelder-Mead. Use these methods with \r\n",
    "#      multiple starting guesses if the problem is \"non-convex\" \r\n",
    "#      so that multiple local optima (i.e., multiple minima) exit.  More \r\n",
    "#      simply, use the Particle Swarm method with upper and lower bounds\r\n",
    "#      for nonconvex problems.  (Adapted from:  \r\n",
    "#       https://julianlsolvers.github.io/Optim.jl/stable/#user/algochoice/)\r\n",
    "\r\n",
    "# 6.  Particle Swarm algorithm is described here:\r\n",
    "\r\n",
    "#        https://en.wikipedia.org/wiki/Particle_swarm_optimization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia nteract 1.6.2",
   "language": "julia",
   "name": "julia-nteract-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "name": "julia",
   "mimetype": "application/julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}