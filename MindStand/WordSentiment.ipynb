{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json; import os; import pandas as pd; import spacy; import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Functions\n",
    " Vader Setiment\\\n",
    " Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentSet = []\n",
    "def sentiment_scores(sentence):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "    # print(\"Overall sentiment dictionary is : \", sentiment_dict)\n",
    "    sentimentSet.append(sentiment_dict)\n",
    "    return sentiment_dict\n",
    "    \n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "sentiment_dict = sid_obj.polarity_scores\n",
    "sentiment_dict(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jason file manipulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting jason files in directory \n",
    "folders = os.listdir(\"edhrec Slack export Sep 4 2016 - Oct 23 2019\"); folderSize = (len(folders)); folderNames = [0]*folderSize\n",
    "ii = 0\n",
    "\n",
    "### Ignoring json files in folder (channels.json, integration_ logs.json, users.json)\n",
    "for count in range(len(folderNames)):\n",
    "\n",
    "    folderNames[ii] = folders[ii]   \n",
    "    ii += 1\n",
    "\n",
    "    if folderNames[ii-1] == \"channels.json\":\n",
    "        folderNames[ii-1] = 0\n",
    "\n",
    "    \n",
    "    if folderNames[ii-1] == \"integration_logs.json\":\n",
    "        folderNames[ii-1] = 0\n",
    "    \n",
    "    if folderNames[ii-1] == \"users.json\":\n",
    "        folderNames[ii-1] = 0\n",
    "###\n",
    "\n",
    "### Removing placed zeros             \n",
    "try:\n",
    "    while True:\n",
    "        folderNames.remove(0)\n",
    "except ValueError:\n",
    "    pass \n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jason files to Python dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = []; folderList = []; folderPath = [0]*len(folderNames); mainPath = []; slackContent = []; all_possible_keys = set(); channelsDict = {'Channel':[]}\n",
    "\n",
    "jj = 0; ii = 0\n",
    "\n",
    "### Conversion of json files to python dictionaries and collection of key value pairs\n",
    "for Paths in folderNames:\n",
    "    \n",
    "    mainPath1 = 'C:/Users/solid/Documents/Scripts/Code/'\n",
    "    mainPath2 = 'MindStand/edhrec Slack export Sep 4 2016 - Oct 23 2019/'\n",
    "    folderPath[ii] = folderNames[ii]\n",
    "    folderList.append(os.path.join(mainPath1,mainPath2,folderPath[ii]))\n",
    "    fileIter = os.listdir(folderList[ii])\n",
    "    \n",
    "    for d in range(len(fileIter)):\n",
    "        fileList.append(fileIter[d])  \n",
    "        mainPath.append(os.path.join(folderList[ii],fileList[d]))\n",
    "        mainPath[jj] = mainPath[d].replace('\\\\','/')\n",
    "        with open(mainPath[ii], encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            slackContent.append(data)\n",
    "            for d in data:\n",
    "                channelsDict['Channel'].append(Paths)\n",
    "                for k in d.keys():\n",
    "                    all_possible_keys.add(k)\n",
    "\n",
    "        jj += 1\n",
    "    ii += 1\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creation of dataframe based on a key: [list of values] organization\n",
    "main_data = {}\n",
    "for stuff in slackContent:\n",
    "    for p in stuff:\n",
    "        for field in list(all_possible_keys):\n",
    "            if field not in main_data:\n",
    "                main_data[field] = []\n",
    "            if field in p:\n",
    "                main_data[field].append(p[field])\n",
    "            else:\n",
    "                main_data[field].append(None)\n",
    "dataframe = pd.DataFrame.from_dict(main_data)\n",
    "df = dataframe[['text','thread_ts','user']]\n",
    "channelDataFrame = pd.DataFrame.from_dict(channelsDict)\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Iterating through dataframe and applying spacy\n",
    "dataSpan = range(0,len(df),1)\n",
    "test_nlp = [0]*len(dataSpan)\n",
    "ii = 0\n",
    "for row in dataSpan:\n",
    "    test_nlp[ii] = nlp(df.text[row])\n",
    "    ii += 1 \n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hitmen', 'killers']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in test_nlp[1000] if token.pos_ == \"NOUN\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Noun, Verb, and Adjective sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDataList = []\n",
    "ii = 0\n",
    "for row in test_nlp:\n",
    "    sentence = [row.text]\n",
    "    noun = [token.text for token in test_nlp[ii] if token.pos_ == \"NOUN\"]\n",
    "    Verb = [token.text for token in test_nlp[ii] if token.pos_ == \"VERB\"]\n",
    "    Adj = [token.text for token in test_nlp[ii] if token.pos_ == \"ADJ\"]\n",
    "    pt1 = sentence\n",
    "    pt2 = noun\n",
    "    pt1_pt2 = sentence,noun\n",
    "    wordDataList.append([pt1_pt2])\n",
    "    iterDataFrame = pd.DataFrame(wordDataList[0], columns=['Sentence','Nouns'])\n",
    "    # wordDataFrame = DataFrames.append(iterDataFrame)\n",
    "    wordDataFrame = pd.DataFrame().append(iterDataFrame)\n",
    "    \n",
    "    ii+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDataList = []\n",
    "iterDataFrame = [0]*len(test_nlp)\n",
    "ii = 0\n",
    "for row in test_nlp:\n",
    "    Sentence = [row.text]\n",
    "    Noun = [token.text for token in test_nlp[ii] if token.pos_ == \"NOUN\"]\n",
    "    Verb = [token.text for token in test_nlp[ii] if token.pos_ == \"VERB\"]\n",
    "    Adj = [token.text for token in test_nlp[ii] if token.pos_ == \"ADJ\"]\n",
    "    pt1 = sentence\n",
    "    pt2 = Noun\n",
    "    pt3 = Verb\n",
    "    pt4 = Adj\n",
    "    pts = Sentence,Noun,Verb,Adj\n",
    "    wordDataList.append(pts)\n",
    "    ii+=1\n",
    "\n",
    "DataFrame = pd.DataFrame(wordDataList,columns=['Sentence','Noun','Verb','Adjective'])\n",
    "wordDataFrame = DataFrame.join(channelDataFrame).join(dataframe['user'])\n",
    "wordDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List items as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_alt_list(list_):\n",
    "    list_ = list_.replace(', ', '\",\"')\n",
    "    list_ = list_.replace('[', '[\"')\n",
    "    list_ = list_.replace(']', '\"]')\n",
    "    return list_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordSentimentDataFrame= wordDataFrame\n",
    "WordSentimentDataFrame = WordSentimentDataFrame.explode('Sentence')\n",
    "WordSentimentDataFrame['Sentence'] = WordSentimentDataFrame['Sentence'].str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordSentimentDataFrame['Sentence'] = WordSentimentDataFrame.loc[WordSentimentDataFrame['Sentence'].str.contains(r'https')==False]\n",
    "WordSentimentDataFrame = WordSentimentDataFrame.loc[WordSentimentDataFrame['Sentence'].str.contains(r'@')==False]\n",
    "WordSentimentDataFrame = WordSentimentDataFrame.loc[WordSentimentDataFrame['Sentence'].str.contains(r'::')==False]\n",
    "WordSentimentDataFrame = WordSentimentDataFrame.loc[WordSentimentDataFrame['Sentence'].str.contains(r'\\\\')==False]\n",
    "WordSentimentDataFrame = WordSentimentDataFrame.loc[WordSentimentDataFrame['Sentence'].str.contains(r'\\+')==False]\n",
    "WordSentimentDataFrame.dropna(inplace=True)\n",
    "WordSentimentDataFrame['Sentence'].replace('',np.nan,inplace=True)\n",
    "WordSentimentDataFrame.dropna(subset=['Sentence'],inplace=True)\n",
    "print(len(wordDataFrame))\n",
    "print(len(WordSentimentDataFrame))\n",
    "WordSentimentDataFrame.drop_duplicates(subset=[\"Sentence\"],inplace=True)\n",
    "WordSentimentDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordSentimentDataFrame.Verb=WordSentimentDataFrame.Verb.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "WordSentimentDataFrame.Adjective=WordSentimentDataFrame.Adjective.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "WordSentimentDataFrame.Noun=WordSentimentDataFrame.Noun.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordSentimentDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "WordSentimentDataFrame['compound'] = [analyzer.polarity_scores(x)['compound'] for x in WordSentimentDataFrame[\"Sentence\"]]\n",
    "WordSentimentDataFrame['neg'] = [analyzer.polarity_scores(x)['neg'] for x in WordSentimentDataFrame[\"Sentence\"]]\n",
    "WordSentimentDataFrame['neu'] = [analyzer.polarity_scores(x)['neu'] for x in WordSentimentDataFrame[\"Sentence\"]]\n",
    "WordSentimentDataFrame['pos'] = [analyzer.polarity_scores(x)['pos'] for x in WordSentimentDataFrame[\"Sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordSentimentDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordSentimentDataFrame.explode('Noun').groupby('Noun').size().plot(kind='bar')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4e6d9786dc1ab3736f85b775378532d8fe18afb1540be17e5c41974a5249fde"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
